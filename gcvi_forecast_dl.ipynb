{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPKitOhXwoQvSDD838qmVm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/issmythe/CS194App/blob/master/gcvi_forecast_dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "RRJPvC7-4NPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installations\n",
        "!pip install fsspec kaleido gcsfs geopandas &> /dev/null"
      ],
      "metadata": {
        "id": "FFebTZJVJHgy",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "w4VnsUrsAElW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71f0e9c-7f2d-4934-ef68-871afde6f084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Authentications\n",
        "from google.colab import auth, drive\n",
        "\n",
        "auth.authenticate_user()\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ! gcloud auth login\n",
        "# ! gcloud auth application-default login"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9yBYkuSrHd5Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import itertools\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# import geopandas as gpd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Plotting\n",
        "import plotly\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import plotly.subplots\n",
        "\n",
        "# Analysis\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import cross_val_predict, HalvingGridSearchCV\n",
        "\n",
        "# Helpers\n",
        "CODE_PATH = 'drive/MyDrive/code/in_season/'\n",
        "sys.path.append(CODE_PATH)\n",
        "sys.path.append('content/' + CODE_PATH)\n",
        "\n",
        "from data_utils import *\n",
        "from model_utils import *\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "u_Db6qxEAXRl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Constants and misc.\n",
        "get_rmse = lambda x, y: mean_squared_error(x, y, squared=False)\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "FIG_PATH = 'drive/MyDrive/figs/in_season/%s/' % datetime.today().strftime('%Y%m')\n",
        "! mkdir -p $FIG_PATH"
      ],
      "metadata": {
        "id": "PiSJxkKTAXO1",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data and linear model helpers"
      ],
      "metadata": {
        "id": "vD7abrhHcRo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Setup"
      ],
      "metadata": {
        "id": "c-xhEZ3GGK70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GEN_DATA = False"
      ],
      "metadata": {
        "id": "0k5zT4GeEVIh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Read in data\n",
        "if GEN_DATA:\n",
        "    gcvi_obs = read_vi_ts(\n",
        "        'gs://nsf-phase2/vi_timeseries/s2_%d_timeseries_sr_corrected_noclouds.csv', 'GCVI')\n",
        "else:\n",
        "    gcvi_obs = pd.read_csv(\n",
        "        'gs://nsf-phase2/vi_timeseries/kenya_all_years/sample_years_no_clouds.csv')\n",
        "    gcvi_obs = gpd.GeoDataFrame(\n",
        "        gcvi_obs, geometry=gpd.points_from_xy(gcvi_obs['x'], gcvi_obs['y']))\\\n",
        "        .drop(['x', 'y'], axis=1)\n",
        "    gcvi_obs['ts'] = pd.to_datetime(gcvi_obs['ts'])\n",
        "\n",
        "vi_max_df = pd.read_csv(\n",
        "    'gs://nsf-phase2/in_season/precomputed_features/partial_season_vis_filtered.csv')\n",
        "\n",
        "pred_df_full = pd.read_csv(\n",
        "    'gs://nsf-phase2/in_season/precomputed_features/full_season_filtered.csv')\n",
        "ids = pred_df_full[['id']]\n",
        "\n",
        "def get_id_from_tuple(x):\n",
        "    lon, lat, year, _ = x\n",
        "    data = [int(year), round(lat, 6), round(lon, 6)]\n",
        "    return '_'.join([str(x) for x in data])\n",
        "\n",
        "gcvi_obs['id'] = gcvi_obs.apply(get_vi_id, axis=1)\n",
        "gcvi_obs['id'] = gcvi_obs['id'].apply(get_id_from_tuple)\n",
        "gcvi_obs = gcvi_obs[['id', 'year', 'ts', 'GCVI']].rename({'GCVI': 'gcvi'}, axis=1).drop_duplicates()\n"
      ],
      "metadata": {
        "id": "gz8zkDlxAXLe",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get daily GCVI df, last observed value filled forward\n",
        "\n",
        "def vis_all_dates_ffill(df):\n",
        "    df['t'] = \\\n",
        "        df['ts'].dt.dayofyear - df['year'].apply(lambda x: datetime(x, 3, 1)).dt.dayofyear\n",
        "\n",
        "    dates = pd.DataFrame({'t': [x for x in range(df['t'].min(), df['t'].max() + 1)]})\n",
        "    vi_all = ids.merge(dates, how='cross').merge(df, how='left')\n",
        "    vi_all = vi_all.sort_values('t')\n",
        "    vi_all['gcvi'] = vi_all.groupby('id')['gcvi'].fillna(method='ffill')\n",
        "    return vi_all, dates\n",
        "\n",
        "# Get column with dates of most recent Sentinel reading for field\n",
        "def add_last_obs_date(df):\n",
        "    df['last_obs'] = df['ts']\n",
        "    df.loc[~df['last_obs'].isna(), 'last_obs'] = \\\n",
        "        df.loc[~df['last_obs'].isna(), 't']\n",
        "    df['last_obs'] = df.groupby('id')['last_obs'].fillna(method='ffill')\n",
        "    df['last_obs'] = (df['last_obs'] - df['last_obs'].min()).fillna(0)\n",
        "    return df\n",
        "\n",
        "def add_cumulative_max(df, end_t=None):\n",
        "    if end_t is None:\n",
        "        start_date = datetime(2022, 3, 1)\n",
        "        end_t = (datetime(2022, 9, 1) - start_date).days\n",
        "\n",
        "    # Filter for growing season and get cumulative maximum\n",
        "    df = df[(df['t'] >= 0) & (df['t'] < end_t)]\n",
        "    df['gcvi_curr_season'] = df['gcvi']\n",
        "    df.loc[df['last_obs'] < 59, 'gcvi_curr_season'] = None\n",
        "    df['cum_max'] = df.groupby('id')['gcvi'].cummax()\n",
        "    df['cum_max_season'] = df.groupby('id')['gcvi_curr_season'].cummax().fillna(df['cum_max'])\n",
        "\n",
        "    # Get mean GCVI/GCVI cumulative max by day of year\n",
        "    daily_avg = df.groupby('t')['gcvi'].mean().reset_index()\\\n",
        "                    .rename({'gcvi': 'gcvi_daily_avg'}, axis=1)\\\n",
        "                    .sort_values('t')\n",
        "    daily_avg['cum_max_daily_avg'] = daily_avg['gcvi_daily_avg'].cummax()\n",
        "    df = df.merge(daily_avg)\n",
        "\n",
        "    # Fill in missing values with daily maximums\n",
        "    df['gcvi'] = df['gcvi'].fillna(df['gcvi_daily_avg'])\n",
        "    df['cum_max'] = df['cum_max'].fillna(df['cum_max_daily_avg'])\n",
        "    df['cum_max_season'] = df['cum_max_season'].fillna(df['cum_max_daily_avg'])\n",
        "    return df\n",
        "\n",
        "def get_forward_interp_vi_df(obs_df, end_t=None):\n",
        "    all_dates, dates = vis_all_dates_ffill(obs_df)\n",
        "    all_dates = add_last_obs_date(all_dates)\n",
        "    cum_max_df = add_cumulative_max(all_dates, end_t)\n",
        "\n",
        "    # Add true max GCVI (dependent var) and clean up year columns\n",
        "    cum_max_df = cum_max_df.merge(vi_max_df[['id', 'gcvi_max_full']])\n",
        "    cum_max_df['year'] = cum_max_df['id'].apply(lambda x: int(x.split('_')[0]))\n",
        "\n",
        "    return cum_max_df.drop_duplicates(), dates\n",
        "\n",
        "gcvi_all, dates = get_forward_interp_vi_df(gcvi_obs)"
      ],
      "metadata": {
        "id": "g4FUQfZVPati",
        "cellView": "form"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6rRh1_QD8W4q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Read in Fourier time series\n",
        "harmonic_ts = pd.read_csv('gs://nsf-phase2/harmonics_10iters/GCVI_harmonics.csv')\n",
        "harmonic_ts = harmonic_ts[['DATE', 'GCVI_HARMFIT', 'id']]\\\n",
        "    .rename({'DATE': 'date', 'GCVI_HARMFIT': 'gcvi'}, axis=1)\n",
        "\n",
        "harmonic_ts['date'] = pd.to_datetime(harmonic_ts['date'])\n",
        "harmonic_ts['year'] = harmonic_ts['date'].dt.year\n",
        "harmonic_ts['doy'] = harmonic_ts['date'].dt.dayofyear\n",
        "harmonic_ts = harmonic_ts.sort_values('date')\n",
        "\n",
        "hist_id_df = harmonic_ts[['id']].drop_duplicates()\n",
        "hist_id_df['cleaned_id'] = hist_id_df['id'].apply(get_id_tuple).apply(get_id_from_tuple)\n",
        "harmonic_ts = harmonic_ts.merge(hist_id_df).drop('id', axis=1).rename({'cleaned_id': 'id'}, axis=1)\n",
        "harmonic_ts = harmonic_ts.merge(vi_max_df[['id', 'gcvi_max_full']])\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "P31qC293C2HN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QuxDsX0ScU4o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ** Helpers"
      ],
      "metadata": {
        "id": "q-JDjSI5zaCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get plot labels\n",
        "start_date = datetime(2022, 3, 1)\n",
        "end_t = (datetime(2022, 9, 1) - start_date).days\n",
        "\n",
        "dates['date'] = dates['t'].apply(lambda x: start_date + timedelta(x))\n",
        "dates = dates[(dates['t'] >= 0) & (dates['t'] < end_t)]\n",
        "labels = dates.assign(day=dates['date'].dt.day, month=dates['date'].dt.month)\n",
        "labels = labels[labels['day'] == 15]\n",
        "labels['label'] = labels.apply(lambda x: f\"{x['month']}/{x['day']}\", axis=1)\n"
      ],
      "metadata": {
        "id": "qDAMFq_Y_Nbi",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper to get cross-validated predictions from linear regression\n",
        "def quick_predict(df, pred_cols, use_window=True):\n",
        "    def _predict_group(g):\n",
        "        g = g.reset_index(drop=True)\n",
        "        group_kfold = GroupKFold(n_splits=4).split(g, groups=g['year'])\n",
        "        group_preds = cross_val_predict(\n",
        "            linear_model.LinearRegression(), g[pred_cols], g['gcvi_max_full'], cv=group_kfold)\n",
        "        return group_preds, g['gcvi_max_full']\n",
        "\n",
        "    if use_window:\n",
        "        df = df[df['t'] < end_t]\n",
        "    preds = df.groupby('t').apply(_predict_group).reset_index()\n",
        "    preds['r2'] = preds[0].apply(lambda x: pearsonr(x[0], x[1])[0] ** 2)\n",
        "    preds['rmse'] = preds[0].apply(lambda x: get_rmse(x[0], x[1]))\n",
        "    return preds.drop(0, axis=1)\n",
        "\n",
        "def get_historical_preds(df, pred_col, filter_test=False):\n",
        "    if filter_test:\n",
        "        df = df[df['year'] == df['obs_year']]\n",
        "    hist_r2 = pearsonr(df[pred_col], df['gcvi_max_full'])[0] ** 2\n",
        "    hist_rmse = ((df[pred_col] - df['gcvi_max_full']) ** 2).mean() ** 0.5\n",
        "    return baseline_preds[['t']].assign(r2=hist_r2, rmse=hist_rmse)\n",
        "\n",
        "def format_output_reduced_form(test_preds, expected_df=None):\n",
        "    expected_df = expected_df if expected_df is not None else gcvi_all.merge(hist_max)\n",
        "    pred_df = expected_df.merge(pd.concat(test_preds)[['id', 't', 'pred']], how='left')\n",
        "    pred_df['cont_pred'] = \\\n",
        "        pred_df.groupby(['id'])['pred'].fillna(method='ffill').fillna(pred_df['hist_max'])\n",
        "\n",
        "    quick_r2 = lambda g: pearsonr(g['cont_pred'], g['gcvi_max_full'])[0] ** 2\n",
        "    quick_rmse = lambda g: get_rmse(g['cont_pred'], g['gcvi_max_full'])\n",
        "    r2_df = pred_df.groupby('t').apply(quick_r2).reset_index().rename({0: 'r2'}, axis=1)\n",
        "    rmse_df = pred_df.groupby('t').apply(quick_rmse).reset_index().rename({0: 'rmse'}, axis=1)\n",
        "    return r2_df.merge(rmse_df)\n",
        "\n",
        "def add_n_since_max(df):\n",
        "    df = df.assign(lt_max=0, group=0)\n",
        "    df.loc[df['gcvi'] < df['cum_max'], 'lt_max'] = 1\n",
        "    df.loc[np.abs(df['gcvi'] - df['cum_max']) < 1e-6, 'group'] = 1\n",
        "\n",
        "    df['group'] = df.groupby('id')['group'].cumsum()\n",
        "    df['n_since_max'] = df.groupby(['id', 'group'])['lt_max'].cumsum()\n",
        "    return df\n",
        "\n",
        "def predict_rolling_window(pred_df, cols, window_size=5, expected_df=None,\n",
        "                           filter_test=False, get_coef=False):\n",
        "    obs_df = pred_df.dropna()\n",
        "    test_preds, coefficients = [], []\n",
        "\n",
        "    for year in range(2016, 2020):\n",
        "        for t in range(end_t):\n",
        "            train = obs_df[(obs_df['year'] != year) & (np.abs(obs_df['t'] - t) <= window_size)]\n",
        "            test = obs_df[(obs_df['year'] == year) & (obs_df['t'] == t)]\n",
        "            if filter_test:\n",
        "                test = test[test['year'] == test['obs_year']]\n",
        "\n",
        "            if len(test) > 0:\n",
        "                lm = linear_model.LinearRegression().fit(train[cols], train['gcvi_max_full'])\n",
        "                test_preds.append(test.assign(pred=lm.predict(test[cols])))\n",
        "                coefficients.append({'year': year, 't': t,\n",
        "                                     **{col + '_coef': coef for col, coef in zip(cols, lm.coef_)}})\n",
        "    if get_coef:\n",
        "        return format_output_reduced_form(test_preds, expected_df), pd.DataFrame(coefficients)\n",
        "        # return pd.concat(test_preds), pd.DataFrame(coefficients)\n",
        "    return format_output_reduced_form(test_preds, expected_df)\n"
      ],
      "metadata": {
        "id": "m8Cnbtn1Ii3s",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Confidence interval code\n",
        "def bootstrap_ci(df, pred_fn, n=100, verbose=True, fname=None):\n",
        "    if fname:\n",
        "        try:\n",
        "            return pd.read_csv(\n",
        "                f'drive/MyDrive/data/feature_forecasting/confidence_intervals/{fname}.csv')\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "    np.random.seed(123)\n",
        "    t0 = time.time()\n",
        "    preds = []\n",
        "    for i in range(n):\n",
        "        sample_ids = ids.sample(frac=1, replace=True)\n",
        "        sample_df = df.merge(sample_ids)\n",
        "        preds.append(pred_fn(sample_df).assign(iter=i))\n",
        "        if verbose and (i + 1) % 10 == 0:\n",
        "            print(f'{i + 1}/{n} folds complete; {round(time.time() - t0, 1)} elapsed.')\n",
        "\n",
        "    all_preds = pd.concat(preds)\n",
        "    lower_ci = all_preds.groupby('t')[['r2', 'rmse']].quantile(0.025).reset_index()\\\n",
        "        .rename({'r2': 'r2_lower', 'rmse': 'rmse_lower'}, axis=1)\n",
        "    upper_ci = all_preds.groupby('t')[['r2', 'rmse']].quantile(1 - 0.025).reset_index()\\\n",
        "        .rename({'r2': 'r2_upper', 'rmse': 'rmse_upper'}, axis=1)\n",
        "    ci_df = lower_ci.merge(upper_ci)\n",
        "    if fname:\n",
        "        ci_df.to_csv(\n",
        "            f'drive/MyDrive/data/feature_forecasting/confidence_intervals/{fname}.csv', index=False)\n",
        "    return ci_df\n",
        "\n",
        "def quick_predict_historical(df, pred_col):\n",
        "    hist_r2 = pearsonr(df[pred_col], df['gcvi_max_full'])[0] ** 2\n",
        "    hist_rmse = ((df[pred_col] - df['gcvi_max_full']) ** 2).mean() ** 0.5\n",
        "    return pd.DataFrame(dict(t=[t for t in range(end_t)])).assign(r2=hist_r2, rmse=hist_rmse)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7KVNO8Yoh9XY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Updated plotting helpers\n",
        "from matplotlib import colors\n",
        "\n",
        "def quick_plot_results(plot_objects, fname=None, show=True):\n",
        "    fig = plotly.subplots.make_subplots(rows=1, cols=2,\n",
        "            subplot_titles=['R2, Predicted vs. Peak GCVI', 'RMSE, Predicted vs. Peak GCVI'])\n",
        "\n",
        "    for po in plot_objects:\n",
        "        po.plot(fig)\n",
        "\n",
        "    rmse_max = max([po.preds['rmse'].max() for po in plot_objects])\n",
        "    fig.update_xaxes(title='Date', tickvals=labels['t'], ticktext=labels['label'])\n",
        "    fig.update_yaxes(range=[0, 1], row=1, col=1)\n",
        "    fig.update_yaxes(range=[0, rmse_max * 1.02], row=1, col=2)\n",
        "\n",
        "    fig.update_layout(height=500, width=1000)\n",
        "    if fname:\n",
        "        plotly.io.write_image(fig, f'{FIG_PATH}/{fname}.png', scale=2)\n",
        "    if show:\n",
        "        fig.show()\n",
        "\n",
        "class PlotObject(object):\n",
        "    def __init__(self, preds, name, kwargs, cis=None):\n",
        "        super(object, self).__init__()\n",
        "        self.preds = preds\n",
        "        self.name = name\n",
        "        self.kwargs = kwargs\n",
        "        self.cis = cis\n",
        "\n",
        "    def get_ci_rgba_str(self, kwargs):\n",
        "        r, g, b, _ = colors.to_rgba(kwargs['line_color'])\n",
        "        return f'rgba({r},{g},{b},0.2)'\n",
        "\n",
        "    def add_prediction_to_plot(self, fig, df, name, kwargs, row, cis=None):\n",
        "        fig.add_trace(\n",
        "            go.Scatter(name=name, x=df['t'], y=df['r2'], **kwargs), row=row, col=1)\n",
        "        fig.add_trace(\n",
        "            go.Scatter(showlegend=False, x=df['t'], y=df['rmse'], **kwargs), row=row, col=2)\n",
        "\n",
        "        def _add_ci(cis, var, col):\n",
        "            if cis is None:\n",
        "                return\n",
        "\n",
        "            fig.add_trace(go.Scatter(\n",
        "                    x=list(cis['t']) + list(cis['t'][::-1]),\n",
        "                    y=list(cis[f'{var}_upper']) + list(cis[f'{var}_lower'][::-1]),\n",
        "                    fill='toself',\n",
        "                    fillcolor=self.get_ci_rgba_str(kwargs),\n",
        "                    line=dict(color='rgba(255,255,255,0)'),\n",
        "                    showlegend=False), row=row, col=col)\n",
        "\n",
        "        _add_ci(cis, 'r2', 1)\n",
        "        _add_ci(cis, 'rmse', 2)\n",
        "\n",
        "    def plot(self, fig, row=1):\n",
        "        self.add_prediction_to_plot(fig, self.preds, self.name, self.kwargs, row=row, cis=self.cis)\n",
        "\n"
      ],
      "metadata": {
        "id": "IopdkGMuiAsc",
        "cellView": "form"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-_vS0qefFZ1l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ** Get baseline preds and historical data"
      ],
      "metadata": {
        "id": "TUr22mjxzUh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get df of historical VI curves\n",
        "nharmonics, omega = 2, 1\n",
        "\n",
        "if GEN_DATA:\n",
        "    # https://colab.research.google.com/drive/1l4LcW0_eD8wjFnv6k-Q2FWy1daXcFnFR?authuser=2#scrollTo=iVxNsOppm_W6\n",
        "    hist_harms = pd.concat([\n",
        "        pd.read_csv(f'gs://nsf-phase2/harmonics_1iter/kenya/{y}.csv').assign(im_year=y)\n",
        "        for y in range(2016, 2023)]) # TODO\n",
        "    hist_harms['year'] = hist_harms['id'].apply(lambda x: int(x.split('_')[0]))\n",
        "\n",
        "    def get_id(row):\n",
        "        return '_'.join([str(row['year']),\n",
        "                         str(round(row['PTLAT'], 6)),\n",
        "                         str(round(row['PTLON'], 6))])\n",
        "\n",
        "    def fourier(t, *coeffs):\n",
        "        vi_fit = coeffs[0] + coeffs[1] * t\n",
        "        for n in range(nharmonics):\n",
        "            timerad = t * 2 * (n + 1) * np.pi * omega\n",
        "            vi_fit += coeffs[2 + n*2] * np.cos(timerad) + coeffs[3 + n*2] * np.sin(timerad)\n",
        "        return vi_fit\n",
        "\n",
        "    def get_fourier(row):\n",
        "        order = ['fourier_c', 'fourier_t', 'fourier_cos1',\n",
        "                'fourier_sin1', 'fourier_cos2', 'fourier_sin2']\n",
        "        params = [row[var] for var in order]\n",
        "        return lambda t: fourier(t, *params)\n",
        "\n",
        "    start = (datetime(2017, 3, 1) - datetime(2017, 1, 1)).days\n",
        "    end = (datetime(2017, 11, 1) - datetime(2017, 1, 1)).days\n",
        "\n",
        "    def get_vi_ts(row):\n",
        "        fourier_fn = get_fourier(row)\n",
        "        return [fourier_fn(i / 365) for i in range(end - start)]\n",
        "\n",
        "    hist_harms = hist_harms[hist_harms['id'].isin(ids['id'])]\n",
        "    hist_harms['year_id'] = hist_harms['id'] + '_' + hist_harms['im_year'].apply(str)\n",
        "    hist_harms = hist_harms.drop_duplicates(\n",
        "        subset=[x for x in hist_harms.columns if x != 'system:index'])\n",
        "    hist_harms['vi_ts'] = hist_harms.apply(get_vi_ts, axis=1)\n",
        "    hist_harms = hist_harms[['year_id', 'vi_ts']].explode('vi_ts').assign(\n",
        "        doy=[x for x in range(start, end)] * hist_harms['year_id'].nunique())\n",
        "\n",
        "    hist_harms['year'] = hist_harms['year_id'].apply(lambda x: int(x.split('_')[0]))\n",
        "    hist_harms['im_year'] = hist_harms['year_id'].apply(lambda x: int(x.split('_')[-1]))\n",
        "\n",
        "else:\n",
        "    hist_harms = pd.read_csv('gs://nsf-phase2/harmonics_1iter/kenya/all_years_by_day.csv')\n",
        "\n",
        "hist_id_to_id = hist_harms[['year_id']].drop_duplicates()\n",
        "hist_id_to_id['id'] = hist_id_to_id['year_id'].apply(lambda x: x[:-5])\n",
        "hist_harms = hist_harms.merge(hist_id_to_id)\n",
        "\n",
        "hist_harms['vi_orig'] = hist_harms['vi_ts']\n",
        "hist_harms.loc[hist_harms['vi_ts'] < gcvi_all['gcvi'].min(), 'vi_ts'] = gcvi_all['gcvi'].min()\n",
        "hist_harms.loc[hist_harms['vi_ts'] > gcvi_all['gcvi'].max(), 'vi_ts'] = gcvi_all['gcvi'].max()\n",
        "\n",
        "hist_harms_med = hist_harms[hist_harms['year'] != hist_harms['im_year']]\\\n",
        "    .groupby(['id', 'doy'])['vi_ts'].median().reset_index().rename({'vi_ts': 'vi_ts_med'}, axis=1)\n",
        "hist_harms_mean = hist_harms[hist_harms['year'] != hist_harms['im_year']]\\\n",
        "    .groupby(['id', 'doy'])['vi_ts'].mean().reset_index().rename({'vi_ts': 'vi_ts_mean'}, axis=1)\n",
        "hist_harms_avg = hist_harms_med.merge(hist_harms_mean)\n",
        "hist_harms_avg['vi_ts'] = hist_harms_avg['vi_ts_med']"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Htz_R-U_JexN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get historical OOS maximum\n",
        "# Constrain to OOS years\n",
        "oos_max = hist_harms[hist_harms['year'] != hist_harms['im_year']]\n",
        "\n",
        "# Get in-season maximum\n",
        "max_start = (datetime(2023, 4, 1) - datetime(2023, 1, 1)).days\n",
        "max_end = (datetime(2023, 9, 1) - datetime(2023, 1, 1)).days\n",
        "hist_max = oos_max[(oos_max['doy'] >= max_start) & (oos_max['doy'] < max_end)]\n",
        "hist_max = hist_max.merge(hist_max.groupby('year_id')['vi_ts'].max().reset_index())\n",
        "\n",
        "# Remove unrealistic values\n",
        "lower, upper = pred_df_full['gcvi_max'].min(), pred_df_full['gcvi_max'].max()\n",
        "hist_max = hist_max[(hist_max['vi_ts'] >= lower) & (hist_max['vi_ts'] <= upper)]\n",
        "\n",
        "# Get mean of peaks and fill na with OOS average\n",
        "fill_na_max = pd.DataFrame({'year': [y for y in range(2016, 2020)]}).assign(\n",
        "    oos_mean_max=[hist_max.loc[hist_max['im_year'] != y, 'vi_ts'].mean()\n",
        "                  for y in range(2016, 2020)])\n",
        "hist_max['id'] = hist_max['year_id'].apply(lambda x: x[:-5])\n",
        "hist_max = hist_max.groupby('id')['vi_ts'].mean().reset_index()\n",
        "hist_max = hist_max.merge(\n",
        "    harmonic_ts[['id', 'year', 'gcvi_max_full']].drop_duplicates(), how='right')\n",
        "hist_max = hist_max.merge(fill_na_max)\n",
        "hist_max['vi_ts'] = hist_max['vi_ts'].fillna(hist_max['oos_mean_max'])\n",
        "hist_max = hist_max.rename({'vi_ts': 'hist_max'}, axis=1)\n",
        "print(round(pearsonr(hist_max['gcvi_max_full'], hist_max['hist_max'])[0] ** 2, 3))\n",
        "gcvi_all = gcvi_all.merge(hist_max)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xvH0078uwpjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c9c64f-bf33-402d-8334-13338e20021e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get baseline and combo predictions for comparison\n",
        "baseline_preds = quick_predict(gcvi_all, ['cum_max'])\n",
        "combo_preds = quick_predict(gcvi_all, ['cum_max', 'hist_max'])\n"
      ],
      "metadata": {
        "id": "jh9wihzIPILh",
        "cellView": "form"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GqjO-eZUPIBw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Get all years data"
      ],
      "metadata": {
        "id": "4acbzREeoLFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Read in GCVI all years\n",
        "gcvi_all_years = pd.read_csv(\n",
        "    'gs://nsf-phase2/vi_timeseries/kenya_all_years/processed/all_years.csv').merge(\n",
        "    gcvi_all[['id', 'year']].drop_duplicates().rename({'year': 'obs_year'}, axis=1))\n",
        "gcvi_obs_ay = gcvi_all_years.dropna(subset='ts')\n",
        "gcvi_max_ay = gcvi_obs_ay.groupby(['id', 'year'])['gcvi'].max().reset_index().merge(gcvi_obs_ay)\n",
        "gcvi_max_ay['year_delta'] = np.abs(gcvi_max_ay['year'] - gcvi_max_ay['obs_year'])\n"
      ],
      "metadata": {
        "id": "l1QbyYgqcn-7",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcb7dc12-0fc4-471d-f19d-804a6b545bac"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-b17b9a823191>:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  gcvi_all_years = pd.read_csv(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make prediction dataframe for all years\n",
        "sent_max = gcvi_all_years.dropna(subset='ts')\\\n",
        "    .groupby(['id', 'year', 'obs_year'])['gcvi'].max().reset_index()\\\n",
        "    .rename({'year': 'im_year'}, axis=1)\n",
        "sent_max = sent_max.merge(sent_max.groupby(['id'])['gcvi'].mean()\n",
        "    .reset_index().rename({'gcvi': 'mean_max'}, axis=1)).merge(\n",
        "    sent_max.groupby(['id'])['im_year'].nunique()\n",
        "    .reset_index().rename({'im_year': 'n_obs'}, axis=1))\n",
        "sent_max['hist_max'] = (\n",
        "    sent_max['mean_max'] * sent_max['n_obs'] - sent_max['gcvi']) / (sent_max['n_obs'] - 1)\n",
        "\n",
        "all_years_pred_df = gcvi_all_years.rename({'gcvi_max_full': 'gcvi_max_ins'}, axis=1)\\\n",
        "    .merge(sent_max.drop(['mean_max', 'n_obs'], axis=1)\\\n",
        "    .rename({'im_year': 'year', 'gcvi': 'gcvi_max_full'}, axis=1))\n",
        "\n",
        "# Use harmonic fit for in-sample maxima\n",
        "idx = (all_years_pred_df['year'] == all_years_pred_df['obs_year'])\n",
        "all_years_pred_df.loc[idx, 'gcvi_max_full'] = all_years_pred_df.loc[idx, 'gcvi_max_ins']\n",
        "\n",
        "max_gcvi = gcvi_all['gcvi_max_full'].max() * 1.1\n",
        "all_years_pred_df = all_years_pred_df[(all_years_pred_df['gcvi_max_full'] > 0) &\n",
        "                                      (all_years_pred_df['hist_max'] > 0) &\n",
        "                                      (all_years_pred_df['gcvi_max_full'] < max_gcvi) &\n",
        "                                      (all_years_pred_df['hist_max'] < max_gcvi)].drop_duplicates()\n"
      ],
      "metadata": {
        "id": "hOHU_AKABhyw",
        "cellView": "form"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del gcvi_all_years, max_gcvi, idx, sent_max, gcvi_obs_ay, gcvi_max_ay\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Grx_6ifCoLrb",
        "outputId": "adb8765a-44a2-411c-a7a6-504feefbfe80"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "152"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hckyDnfmOwUr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workspace"
      ],
      "metadata": {
        "id": "SQNKsGW_gTHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Kmfd6QuOpqOj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-CHBEfEolloZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_t = 60\n",
        "pred_df = gcvi_all[gcvi_all['t'] <= pred_t].groupby(['id', 'year'])['gcvi'].apply(list)\\\n",
        "    .reset_index().merge(gcvi_all[['id', 'gcvi_max_full']].drop_duplicates())\n",
        "\n",
        "pred_df = all_years_pred_df[all_years_pred_df['t'] <= pred_t]\\\n",
        "    .groupby(['id', 'year', 'obs_year'])['gcvi'].apply(list)\\\n",
        "    .reset_index().merge(gcvi_all[['id', 'gcvi_max_full']].drop_duplicates())\n"
      ],
      "metadata": {
        "id": "4McP3fBKgYwd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O3mC8ruXOmKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train/test + model data helpers\n",
        "def train(model, device, train_loader, optimizer, criterion):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        inputs, labels = x.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return (running_loss / (batch_idx + 1)) ** 0.5\n",
        "\n",
        "\n",
        "def test(model, device, test_x, test_y, criterion, predict=False):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        data = torch.Tensor(test_x).to(device)\n",
        "        target = torch.Tensor(test_y).to(device)\n",
        "        outputs = model(data)\n",
        "        detached = outputs.detach().cpu().numpy().flatten() if device.type == 'cuda' \\\n",
        "            else outputs.detach().numpy().flatten()\n",
        "        if predict:\n",
        "            return detached\n",
        "        # print('*', outputs.shape, target.shape)\n",
        "        mse = criterion(outputs, target).item()\n",
        "        corr = pearsonr(detached, test_y.flatten())[0]\n",
        "\n",
        "    return mse ** 0.5, corr\n",
        "\n",
        "\n",
        "class ModelData:\n",
        "    def __init__(self, pred_df, year, val_year=None, iter=0):\n",
        "        self.ss_x = StandardScaler()\n",
        "        self.ss_y = StandardScaler()\n",
        "        self.train_pd, self.val_pd, self.test_pd = \\\n",
        "            self.get_train_val_test(pred_df, year, iter, val_year)\n",
        "        self.train_x, self.train_y, self.train_ds, self.train_loader = \\\n",
        "            self.get_model_data(self.train_pd, ['gcvi'], True)\n",
        "        self.test_x, self.test_y, self.test_ds, self.test_loader = \\\n",
        "            self.get_model_data(self.test_pd, ['gcvi'], False)\n",
        "        self.val_x, self.val_y, self.val_ds, self.val_loader = \\\n",
        "            self.get_model_data(self.val_pd, ['gcvi'], False)\n",
        "\n",
        "\n",
        "    def get_train_val_test(self, pred_df, year, iter=0, val_year=None):\n",
        "        train_pd = pred_df[pred_df['year'] != year].sample(frac=1, random_state=123)\n",
        "        test_pd = pred_df[pred_df['year'] == year]\n",
        "        if val_year is None:\n",
        "            start = int(iter * 0.2 * len(train_pd))\n",
        "            end = int((iter + 1) * 0.2 * len(train_pd))\n",
        "            return (train_pd.iloc[np.r_[0: start, end: len(train_pd)]],\n",
        "                    train_pd.iloc[start: end],\n",
        "                    test_pd)\n",
        "\n",
        "        return (train_pd[train_pd['year'] != val_year],\n",
        "                pred_df[pred_df['year'] == val_year],\n",
        "                test_pd)\n",
        "\n",
        "    def get_model_data(self, df, ts_cols, train, batch_size=64):\n",
        "        x = np.stack([np.array(df[c].to_list()) for c in ts_cols], axis=1)\n",
        "        n, vars, days = x.shape\n",
        "        x = x.reshape(n, 1, vars, days)\n",
        "\n",
        "        y = np.array(df['gcvi_max_full']).reshape(n, 1)\n",
        "        y = self.ss_y.fit_transform(y) if train else self.ss_y.transform(y)\n",
        "\n",
        "        ds = TensorDataset(torch.Tensor(x).type(torch.float32),\n",
        "                           torch.Tensor(y).type(torch.float32))\n",
        "        loader = DataLoader(ds, batch_size=batch_size)\n",
        "        return x, y, ds, loader"
      ],
      "metadata": {
        "cellView": "form",
        "id": "n0Rky8aL8syo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make net\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __conv_calc(self, in_dim, pad, stride, k):\n",
        "        out = int(np.floor((in_dim + 2 * pad - (k - 1) - 1) / stride + 1))\n",
        "        return out\n",
        "\n",
        "    def __init__(self, x_dim, cdim1=8, ldim=8, dropout=0.1, filter_sz=7, stride=4, verbose=False):\n",
        "        super(Net, self).__init__()\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.conv2d = nn.Conv2d(1, cdim1, (1, filter_sz), stride)\n",
        "        c1_dim = self.__conv_calc(x_dim, 0, stride, filter_sz)\n",
        "\n",
        "        self.dropout1 = nn.Dropout2d(dropout)\n",
        "        flattened_dim = int(c1_dim / 2) * cdim1\n",
        "\n",
        "        self.cnn_fc = nn.Linear(flattened_dim, ldim)\n",
        "        self.fc = nn.Linear(ldim, 1)\n",
        "\n",
        "    def quick_print(self, x, i):\n",
        "        if self.verbose:\n",
        "            print(i, x.shape)\n",
        "        return i + 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        i = self.quick_print(x, 0)\n",
        "\n",
        "        # Convolution #1\n",
        "        x = self.conv2d(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        i = self.quick_print(x, i)  # 1\n",
        "\n",
        "        x = torch.flatten(x, 2)\n",
        "        i = self.quick_print(x, i)  # 2\n",
        "\n",
        "        # Average pool\n",
        "        x = F.max_pool1d(x, 2)\n",
        "        i = self.quick_print(x, i)  # 3\n",
        "        x = torch.flatten(x, 1)\n",
        "        i = self.quick_print(x, i)  # 4\n",
        "\n",
        "        # CNN FC layer\n",
        "        x = self.cnn_fc(x)\n",
        "        x = F.relu(x)\n",
        "        i = self.quick_print(x, i)  # 5\n",
        "\n",
        "        return self.fc(x)  # .flatten()\n",
        "\n",
        "\n",
        "\n",
        "ss_x = StandardScaler()\n",
        "ss_y = StandardScaler()\n",
        "\n",
        "dummy_md = ModelData(pred_df, 2016, val_year=2017)\n",
        "x, _, _, _ = dummy_md.get_model_data(pred_df[:5], ['gcvi'], train=True)\n",
        "\n",
        "my_nn = Net(x.shape[-1], cdim1=2, ldim=16, dropout=0.1, verbose=True)\n",
        "optimizer = optim.SGD(my_nn.parameters(), lr=0.001)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "test_im = torch.from_numpy(x[:2]).type(torch.float32)\n",
        "result = my_nn(test_im)\n",
        "result"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KXMO3dgNgYqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426aa004-575f-481f-b412-dad27c5c666c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([2, 1, 1, 61])\n",
            "1 torch.Size([2, 2, 1, 14])\n",
            "2 torch.Size([2, 2, 14])\n",
            "3 torch.Size([2, 2, 7])\n",
            "4 torch.Size([2, 14])\n",
            "5 torch.Size([2, 16])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2074],\n",
              "        [0.2054]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prediction helpers\n",
        "class TrainObject:\n",
        "    def __init__(self, model_args):\n",
        "\n",
        "        t0 = time.time()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.criterion = nn.MSELoss()\n",
        "        dummy_md = ModelData(pred_df, 2016, val_year=2017)\n",
        "        x, _, _, _ = dummy_md.get_model_data(pred_df, ['gcvi'], train=True)\n",
        "\n",
        "        self.model = Net(x.shape[3], cdim1=model_args['cdim'], ldim=model_args['ldim'],\n",
        "                         dropout=model_args['dropout'], filter_sz=model_args['filter_sz'],\n",
        "                         stride=model_args['stride'], verbose=False)\n",
        "        self.model.to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=model_args['lr'])\n",
        "\n",
        "        self.epochs = model_args['epochs']\n",
        "\n",
        "def do_epoch(md, to, min_rmse, epoch, t0, verbose):\n",
        "    train_rmse = train(to.model, to.device, md.train_loader, to.optimizer, to.criterion)\n",
        "    val_rmse, val_corr = test(to.model, to.device, md.val_x, md.val_y, to.criterion)\n",
        "    if verbose:\n",
        "        test_rmse, test_corr = test(to.model, to.device, md.test_x, md.test_y, to.criterion)\n",
        "        print(f'{epoch + 1} / {to.epochs} complete. Train: {round(train_rmse, 3)}.',\n",
        "            f'Val: {round(val_rmse, 3)} {round(val_corr, 3)}',\n",
        "            f'Test: {round(test_rmse, 3)} {round(test_corr, 3)}',\n",
        "            round(time.time() - t0, 2))\n",
        "    return min(min_rmse, val_rmse), val_rmse, val_corr\n",
        "\n",
        "def quick_predict_year(\n",
        "    year, model_args, iter=0, val_year=None, tol=None, verbose=True, return_objects=False):\n",
        "    # Get data and set up model\n",
        "    md = ModelData(pred_df, year, iter=iter, val_year=val_year)\n",
        "    to = TrainObject(model_args)\n",
        "\n",
        "    # Initialize rmse and correlation\n",
        "    val_rmse, val_corr = test(to.model, to.device, md.val_x, md.val_y, to.criterion)\n",
        "    rmse, corr, min_rmse, n_inc = [val_rmse], [val_corr], val_rmse, 0\n",
        "    if verbose:\n",
        "        print(f'{0} / {max_epochs} complete.', f'Test: {round(val_rmse, 3)} {round(val_corr, 3)}')\n",
        "\n",
        "    # Train\n",
        "    t0 = time.time()\n",
        "    for epoch in range(to.epochs):\n",
        "        min_rmse, val_rmse, val_corr = do_epoch(md, to, min_rmse, epoch, t0, verbose)\n",
        "        rmse, corr = rmse + [val_rmse], corr + [val_corr]\n",
        "\n",
        "        n_inc = n_inc + 1 if val_rmse > min_rmse else 0\n",
        "        if tol is not None and n_inc >= tol:\n",
        "            break\n",
        "\n",
        "    # Predict\n",
        "    if return_objects:\n",
        "        return md, to\n",
        "\n",
        "    preds = test(to.model, to.device, md.test_x, md.test_y, to.criterion, predict=True)\n",
        "    return to.model, md.test_pd[['year', 'obs_year', 'id', 'gcvi_max_full']].assign(\n",
        "        pred=md.ss_y.inverse_transform(preds.reshape(md.test_y.shape)).flatten())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MSpzm7L9Xncl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jyWUFax5xZVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Grid search set-up\n",
        "def get_model_data_rmse(to, md, to_predict):\n",
        "    if to_predict == 'train':\n",
        "        x, y, df = md.train_x, md.train_y, md.train_pd\n",
        "    if to_predict == 'val':\n",
        "        x, y, df = md.val_x, md.val_y, md.val_pd\n",
        "    if to_predict == 'test':\n",
        "        x, y, df = md.test_x, md.test_y, md.test_pd\n",
        "\n",
        "    preds = test(to.model, to.device, x, y, to.criterion, predict=True)\n",
        "    preds = df[['year', 'obs_year', 'id', 'gcvi_max_full']].assign(\n",
        "            pred=md.ss_y.inverse_transform(preds.reshape(y.shape)).flatten())\n",
        "    preds_ins = preds[preds['year'] == preds['obs_year']]\n",
        "    return (get_rmse(preds['gcvi_max_full'], preds['pred']),\n",
        "            get_rmse(preds_ins['gcvi_max_full'], preds_ins['pred']))\n",
        "\n",
        "param_grid = {'cdim': [2, 4, 8, 16],\n",
        "              'ldim': [2, 4, 8, 16],\n",
        "              'dropout': [0, 0.1, 0.2],\n",
        "              'lr': [5e-4, 1e-3, 5e-3],\n",
        "              'filter_sz': [3, 7, 10],\n",
        "              'stride': [2, 4, 6],\n",
        "              'epochs': [25, 50, 100]}\n",
        "\n",
        "keys, values = zip(*param_grid.items())\n",
        "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "N_COMBOS = 1\n",
        "np.random.seed(123)\n",
        "param_combinations = np.random.choice(\n",
        "    param_combinations, len(param_combinations), replace=False)[:N_COMBOS]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "je4YQJ7yxXLs"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Do grid search\n",
        "i = 0\n",
        "t0 = time.time()\n",
        "\n",
        "for test_year in range(2016, 2020):\n",
        "    for val_year in range(test_year + 1, 2020):\n",
        "        results = []\n",
        "        for p in range(len(param_combinations)):\n",
        "            model_params = param_combinations[p]\n",
        "            md, to = quick_predict_year(\n",
        "                test_year, model_params, val_year=val_year, verbose=False, return_objects=True)\n",
        "\n",
        "            row = model_params.copy()\n",
        "            row['test_year'], row['val_year'], row['p'] = test_year, val_year, p\n",
        "            row['train_rmse_all'], row['train_rmse_ins'] = get_model_data_rmse(to, md, 'train')\n",
        "            row['val_rmse_all'], row['val_rmse_ins'] = get_model_data_rmse(to, md, 'val')\n",
        "            row['test_rmse_all'], row['test_rmse_ins'] = get_model_data_rmse(to, md, 'test')\n",
        "            results.append(row)\n",
        "\n",
        "            i += 1\n",
        "            print(f'{test_year}, {val_year}, {i} / {N_COMBOS} complete',\n",
        "                  f'{round((time.time() - t0) / 60, 1)} minutes elapsed')\n",
        "\n",
        "        pd.DataFrame(results).to_csv(\n",
        "            f'gs://nsf-phase2/in_season/deep_learning/{test_year}_{val_year}.csv', index=False)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JTlEl5LIx4kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Process grid search results\n",
        "dir = 'gs://nsf-phase2/in_season/deep_learning'\n",
        "results = pd.concat([\n",
        "    pd.read_csv(f'{dir}/{y1}_{y2}.csv') for y1 in range(2016, 2020) for y2 in range(y1 + 1, 2020)])\n",
        "\n",
        "counts = pred_df_full.groupby(['year'])['id'].count().reset_index().rename({'id': 'n'}, axis=1)\n",
        "param_cols = ['cdim', 'ldim', 'dropout', 'lr', 'filter_sz', 'stride', 'epochs']\n",
        "rename_dict = {'test_rmse_ins': 'val_rmse_ins', 'test_year': 'val_year', 'val_year': 'test_year'}\n",
        "results = pd.concat([results[param_cols + ['test_year', 'val_year', 'val_rmse_ins']],\n",
        "    results[param_cols + ['test_year', 'val_year', 'test_rmse_ins']].rename(rename_dict, axis=1)])\\\n",
        "    .merge(counts.rename({'year': 'val_year'}, axis=1))\n",
        "\n",
        "results['mse_w'] = (results['val_rmse_ins'] ** 2) * results['n']\n",
        "grouped_results = results.groupby(param_cols + ['test_year'])[['n', 'mse_w']].sum().reset_index()\n",
        "grouped_results['rmse'] = (grouped_results['mse_w'] / grouped_results['n']) ** 0.5"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7jcUkz9JIRTc"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "c3Qz1BemJT4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MLHx_rg0OHoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GRWiN2lTC2pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "shVYkLls_I-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e5-wzidox4f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rtRVhWpfza1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bFm5W8GpIgG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rzV18xZfImdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5lNbF877wUbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cNRwI17_wUYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ip8m_NSRwUVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Predict with train/val split by year\n",
        "tol, max_epochs = 1, 10\n",
        "\n",
        "obs_years = pred_df[['id', 'obs_year']].drop_duplicates()\n",
        "preds = []\n",
        "\n",
        "for test_year in range(2016, 2020):\n",
        "    for val_year in range(2016, 2022):\n",
        "    # for val_year in range(5):\n",
        "        if test_year == val_year:\n",
        "            continue\n",
        "        # model, iter_preds = quick_predict_year(test_year, val_year=val_year)\n",
        "        model, iter_preds = quick_predict_year(test_year, val_year=val_year, tol=tol, max_epochs=max_epochs)\n",
        "        iter_preds = iter_preds.merge(obs_years[obs_years['obs_year'] == test_year])\n",
        "        preds.append(iter_preds.assign(val_year=val_year))\n",
        "\n",
        "dl_preds = pd.concat(preds).groupby(['year', 'id']).mean().reset_index()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mcmFkKLKBhYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl_preds"
      ],
      "metadata": {
        "id": "L4IwbopPQBPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = dl_preds#[dl_preds['year'] != 2019]\n",
        "r2 = pearsonr(df['gcvi_max_full'], df['pred'])[0] ** 2\n",
        "rmse = ((df['gcvi_max_full'] - df['pred']) ** 2).mean() ** 0.5\n",
        "# (0.4326105359863152, 1.374710508448396)\n",
        "r2, rmse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5GIgXMjgYiG",
        "outputId": "f0515df1-889c-48ef-fe1a-3654c6be9d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4091454791476172, 1.4085011226107258)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ZYImwRPdT8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x_3HO1UbbiSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year = 2017\n",
        "train_pd, val_pd, test_pd = get_train_val_test(year)\n",
        "train_x, train_y, train_ds, train_loader = get_model_data(train_pd, ss_x, ss_y, ['gcvi'], True)\n",
        "test_x, test_y, test_ds, test_loader = get_model_data(test_pd, ss_x, ss_y, ['gcvi'], False)\n",
        "val_x, val_y, val_ds, val_loader = get_model_data(val_pd, ss_x, ss_y, ['gcvi'], False)\n",
        "\n",
        "dfx, dfy, dfpd = train_x, train_y, train_pd\n",
        "dfx, dfy, dfpd = val_x, val_y, val_pd\n",
        "dfx, dfy, dfpd = test_x, test_y, test_pd\n",
        "\n",
        "plot_preds = test(model, device, dfx, dfy, criterion, predict=True)\n",
        "plot_preds = dfpd[['year', 'id', 'gcvi_max_full']].assign(\n",
        "    pred=ss_y.inverse_transform(plot_preds.reshape(dfy.shape)).flatten())\n",
        "\n",
        "print(round(pearsonr(plot_preds['gcvi_max_full'], plot_preds['pred'])[0] ** 2, 3),\n",
        "      round(((plot_preds['gcvi_max_full'] - plot_preds['pred']) ** 2).mean() ** 0.5, 3))\n",
        "print(baseline_preds[baseline_preds['t'] == pred_t])\n",
        "\n",
        "# sns.scatterplot(x=preds['gcvi_max_full'], y=preds['pred'])\n",
        "sns.scatterplot(x=plot_preds['gcvi_max_full'], y=plot_preds['pred'])"
      ],
      "metadata": {
        "id": "ouXVbbJVGay9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovH8BdEyBLuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(baseline_preds[baseline_preds['t'] == pred_t])\n",
        "sns.scatterplot(x=baseline_preds['t'], y=baseline_preds['r2'])"
      ],
      "metadata": {
        "id": "pm7gfWh5nEiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZnYXcsMunEdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bmkCy63nn8-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "70phJ6oXn874"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NfOFSHIXn85b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kcwHDokxn82L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZKheHzfGn8zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qEneDJK1n8wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scratch"
      ],
      "metadata": {
        "id": "FE2UryNpn9VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Grid search test run\n",
        "# cdim, ldim, dropout, lr, filter_sz, filter_stride, epochs\n",
        "\n",
        "def get_model_data_rmse(to, md, to_predict):\n",
        "    if to_predict == 'train':\n",
        "        x, y, df = md.train_x, md.train_y, md.train_pd\n",
        "    if to_predict == 'val':\n",
        "        x, y, df = md.val_x, md.val_y, md.val_pd\n",
        "    if to_predict == 'test':\n",
        "        x, y, df = md.test_x, md.test_y, md.test_pd\n",
        "\n",
        "    preds = test(to.model, to.device, x, y, to.criterion, predict=True)\n",
        "    preds = df[['year', 'obs_year', 'id', 'gcvi_max_full']].assign(\n",
        "            pred=md.ss_y.inverse_transform(preds.reshape(y.shape)).flatten())\n",
        "    preds_ins = preds[preds['year'] == preds['obs_year']]\n",
        "    return (get_rmse(preds['gcvi_max_full'], preds['pred']),\n",
        "            get_rmse(preds_ins['gcvi_max_full'], preds_ins['pred']))\n",
        "\n",
        "param_grid = {'cdim': [2, 4, 8, 16],\n",
        "              'ldim': [2, 4, 8, 16],\n",
        "              'dropout': [0, 0.1, 0.2],\n",
        "              'lr': [5e-4, 1e-3, 5e-3],\n",
        "              'filter_sz': [3, 7, 10],\n",
        "              'stride': [2, 4, 6],\n",
        "              'epochs': [25, 50, 100]}\n",
        "\n",
        "keys, values = zip(*param_grid.items())\n",
        "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "np.random.seed(123)\n",
        "param_combinations = np.random.choice(param_combinations, 100, replace=False)\n",
        "\n",
        "test_year, val_year = 2016, 2017\n",
        "results = []\n",
        "\n",
        "i, t0 = 0, time.time()\n",
        "for model_params in param_combinations:\n",
        "    row = model_params\n",
        "\n",
        "    # try:\n",
        "    md, to = quick_predict_year(\n",
        "        test_year, model_params, val_year=val_year, verbose=False, return_objects=True)\n",
        "    row['train_rmse_all'], row['train_rmse_ins'] = get_model_data_rmse(to, md, 'train')\n",
        "    row['val_rmse_all'], row['val_rmse_ins'] = get_model_data_rmse(to, md, 'val')\n",
        "    row['test_rmse_all'], row['test_rmse_ins'] = get_model_data_rmse(to, md, 'test')\n",
        "    # except:\n",
        "    #     print('!!!!! Params failed', model_params)\n",
        "\n",
        "    results.append(row)\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print('Recording results so far...')\n",
        "        pd.DataFrame(results).to_csv(\n",
        "            'gs://nsf-phase2/in_season/deep_learning/initial_param_performance.csv', index=False)\n",
        "\n",
        "    i += 1\n",
        "    print(f'{i} / 100 complete, {round(time.time() - t0)} elapsed')\n",
        "\n"
      ],
      "metadata": {
        "id": "ZlevpM63xKgC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}